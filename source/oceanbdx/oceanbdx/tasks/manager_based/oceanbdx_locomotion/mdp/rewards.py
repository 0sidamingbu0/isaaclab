# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""Reward functions for OceanBDX locomotion environment."""

from __future__ import annotations

import torch
from typing import TYPE_CHECKING

from isaaclab.assets import Articulation
from isaaclab.managers import SceneEntityCfg
from isaaclab.sensors import ContactSensor


if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv


def lin_vel_x_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Reward for linear velocity along x-axis tracking."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    # get command
    cmd = env.command_manager.get_command("base_velocity")
    # compute the reward
    return torch.square(cmd[:, 0] - asset.data.root_lin_vel_b[:, 0])


def lin_vel_y_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Reward for linear velocity along y-axis tracking."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    # get command
    cmd = env.command_manager.get_command("base_velocity")
    # compute the reward
    return torch.square(cmd[:, 1] - asset.data.root_lin_vel_b[:, 1])


def ang_vel_z_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Reward for angular velocity around z-axis tracking."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    # get command
    cmd = env.command_manager.get_command("base_velocity")
    # compute the reward
    return torch.square(cmd[:, 2] - asset.data.root_ang_vel_b[:, 2])


def lin_vel_z_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Penalty for linear velocity along z-axis (vertical motion)."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    return torch.square(asset.data.root_lin_vel_b[:, 2])


def ang_vel_xy_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Penalty for angular velocity around x and y-axis."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    return torch.sum(torch.square(asset.data.root_ang_vel_b[:, :2]), dim=1)


def flat_orientation_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Penalty for non-flat base orientation."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    # penalize deviation from gravity direction
    return torch.sum(torch.square(asset.data.projected_gravity_b[:, :2]), dim=1)


def dof_torques_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """L2 penalty on joint torques."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    return torch.sum(torch.square(asset.data.applied_torque), dim=1)


def dof_acc_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """L2 penalty on joint accelerations."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    return torch.sum(torch.square(asset.data.joint_acc), dim=1)


def action_rate_l2(env: ManagerBasedRLEnv) -> torch.Tensor:
    """L2 penalty on action differences."""
    return torch.sum(torch.square(env.action_manager.action - env.action_manager.prev_action), dim=1)


def undesired_contacts(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg = SceneEntityCfg("contact_forces"),
    threshold: float = 1.0
) -> torch.Tensor:
    """Penalty for contacts at undesired body parts."""
    # extract the used quantities (to enable type-hinting)
    sensor: ContactSensor = env.scene[sensor_cfg.name]
    # undesired contacts: any contact except feet
    # assume first 2 contacts are feet, rest are undesired
    contact_forces = sensor.data.net_forces_w_history
    # compute contact force magnitude
    contact_force_norm = torch.norm(contact_forces.view(env.num_envs, -1, 3), dim=-1)
    # penalty for undesired contacts (exclude feet contacts)
    # threshold and penalty
    penalty = torch.sum((contact_force_norm > threshold).float(), dim=-1)
    return penalty


def feet_air_time(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg = SceneEntityCfg("contact_forces"),
    command_name: str = "base_velocity",
    threshold: float = 1.0,
) -> torch.Tensor:
    """Reward long step time steps (feet in air)."""
    # extract the used quantities (to enable type-hinting)
    sensor: ContactSensor = env.scene[sensor_cfg.name]
    # get command
    command = env.command_manager.get_command(command_name)[:, :2]
    # compute the reward
    contact_forces = sensor.data.net_forces_w_history
    # compute contact force magnitude for feet
    feet_contact_forces = contact_forces.view(env.num_envs, -1, 3)[:, :2]  # first 2 are feet
    contact_force_norm = torch.norm(feet_contact_forces, dim=-1)
    # check if in contact
    in_contact = contact_force_norm > threshold
    # reward for longer air time when walking
    reward = (1.0 - in_contact.float()) * torch.norm(command, dim=1).unsqueeze(-1)
    return torch.sum(reward, dim=1)


def base_height_l2(
    env: ManagerBasedRLEnv, 
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    target_height: float = 0.4
) -> torch.Tensor:
    """Reward for maintaining base height."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    # compute height error
    base_height = asset.data.root_pos_w[:, 2]
    return torch.square(base_height - target_height)


def energy_expenditure(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Penalty for energy expenditure (torque * velocity)."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    # compute power consumption
    power = torch.abs(asset.data.applied_torque * asset.data.joint_vel)
    return torch.sum(power, dim=1)


def joint_deviation_l1(
    env: ManagerBasedRLEnv, 
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
) -> torch.Tensor:
    """L1 penalty on joint position deviation from default."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    # compute joint deviation
    joint_deviation = torch.abs(asset.data.joint_pos - asset.data.default_joint_pos)
    return torch.sum(joint_deviation, dim=1)


def is_alive(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Reward for staying alive (not terminating)."""
    return 1.0 - env.termination_manager.terminated.float()


def is_healthy(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    min_height: float = 0.2,
    max_height: float = 1.0,
) -> torch.Tensor:
    """Reward for staying healthy (proper height and orientation)."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    # check height bounds
    base_height = asset.data.root_pos_w[:, 2]
    height_ok = (base_height > min_height) & (base_height < max_height)
    # check orientation
    up_proj = asset.data.projected_gravity_b[:, 2]  # z component of gravity in base frame
    orientation_ok = up_proj < -0.5  # cos(120Â°) approximately
    # combine conditions
    healthy = height_ok & orientation_ok
    return healthy.float()


def track_lin_vel_xy_exp(env: ManagerBasedRLEnv, command_name: str, std: float, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Reward for tracking linear velocity commands (xy) using exponential kernel."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    # get command
    cmd = env.command_manager.get_command(command_name)
    # compute velocity error
    vel_error = torch.sum(torch.square(cmd[:, :2] - asset.data.root_lin_vel_b[:, :2]), dim=1)
    return torch.exp(-vel_error / std**2)


def track_ang_vel_z_exp(env: ManagerBasedRLEnv, command_name: str, std: float, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Reward for tracking angular velocity commands (z) using exponential kernel."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    # get command
    cmd = env.command_manager.get_command(command_name)
    # compute velocity error
    vel_error = torch.square(cmd[:, 2] - asset.data.root_ang_vel_b[:, 2])
    return torch.exp(-vel_error / std**2)


def base_pitch_penalty(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Specific penalty for pitch angle (forward/backward bending)."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    # get pitch component (y-axis rotation in base frame)
    pitch_component = asset.data.projected_gravity_b[:, 1]  # gravity y-component in base frame
    return torch.square(pitch_component)


def knee_position_reward(env: ManagerBasedRLEnv,
                        target_angle: float = 0.0,
                        asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Reward for keeping knee joints (leg_l4, leg_r4) close to target angle."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    
    # Get all joint names and find knee joint indices (only once globally)
    if not hasattr(knee_position_reward, '_knee_indices'):
        joint_names = asset.data.joint_names
        knee_indices = []
        
        # Debug: print joint names and indices (only once globally)
        print("Joint names and indices:")
        for i, name in enumerate(joint_names):
            print(f"  {i}: {name}")
        
        # Find knee joint indices
        for i, name in enumerate(joint_names):
            if name in ["leg_l4_joint", "leg_r4_joint"]:
                knee_indices.append(i)
                print(f"Found knee joint: {name} at index {i}")
        
        # Cache the indices globally
        knee_position_reward._knee_indices = knee_indices
        
        if len(knee_indices) == 0:
            print("Warning: No knee joints found!")
    
    # Use cached indices
    knee_indices = knee_position_reward._knee_indices
    
    # If no knee joints found, return zero reward
    if len(knee_indices) == 0:
        return torch.zeros(env.num_envs, device=env.device)
    
    # Get current knee positions
    knee_positions = asset.data.joint_pos[:, knee_indices]
    
    # Calculate deviation from target angle
    knee_error = torch.sum(torch.square(knee_positions - target_angle), dim=1)
    
    # Return reward (exponential kernel)
    return torch.exp(-knee_error / 0.25)  # ÏÂ² = 0.25


def upright_posture_reward(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Reward for maintaining upright posture (body vertical)."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    
    # Get the z-component of projected gravity (should be close to -1 for upright posture)
    upright_component = asset.data.projected_gravity_b[:, 2]
    target_upright = -1.0  # Perfect upright posture
    
    # Reward for being close to upright
    upright_error = torch.square(upright_component - target_upright)
    return torch.exp(-upright_error / 0.1)  # Tight tolerance for upright posture


# Aliases for common reward functions used in complete config
joint_torques_l2 = dof_torques_l2
joint_acc_l2 = dof_acc_l2


def contact_forces(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg = SceneEntityCfg("contact_forces"),
    threshold: float = 0.5,
) -> torch.Tensor:
    """Reward for contact forces at feet (encouraging ground contact)."""
    # extract the used quantities (to enable type-hinting)
    sensor: ContactSensor = env.scene[sensor_cfg.name]
    # get contact forces
    contact_forces = sensor.data.net_forces_w_history
    # compute contact force magnitude
    contact_force_norm = torch.norm(contact_forces.view(env.num_envs, -1, 3), dim=-1)
    # reward for contact above threshold
    contact_reward = (contact_force_norm > threshold).float()
    return torch.sum(contact_reward, dim=-1)


def foot_contact_reward(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg = SceneEntityCfg("contact_forces"),
    min_force: float = 2.0,  # æå°æææ¥è§¦å (N)
    max_force: float = 200.0,  # æå¤§åçæ¥è§¦å (N)ï¼é¿åå¥å±æå»
    target_force: float = 50.0,  # ç®æ æ¥è§¦å (N)ï¼æ­£å¸¸ç«ç«çå
    min_z_ratio: float = 0.2,  # Zååå æ»åçæå°æ¯ä¾ï¼ä½ä¸ºå¥å±çå°ºåº¦å å­
) -> torch.Tensor:
    """
    Reward for proper foot contact with ground - ç®åçï¼åªç¨Zååå¤æ­æ¥è§¦è´¨é.
    
    å¤æ­ç­ç¥ï¼ç®åï¼éç¨äºå¤æå°å½¢ï¼:
    1. æ¥è§¦åå¨åçèå´å (2N-200N)
    2. æ¥è§¦åä¸»è¦åä¸ (Zåæ³ååæ¯ä¾ï¼èµ·å§å¼20%ï¼ä¸å¥å±ææ­£æ¯)
    3. ç»åå¥å±ï¼æ¥è§¦åè´¨é Ã Zååæ¯ä¾ç³»æ°
    
    ä¼å¿ï¼
    - é¿åè§åº¦å¤æ­å¨ä¸å¹³å°é¢çè¯¯å¤
    - ä¸æ³¨äºåçå¤§å°åæ¹åï¼æ´å¯é 
    - éç¨äºåç§å°å½¢åå¡åº¦
    - å¹³æ»å¥å±ï¼æ¯ä¾è¶é«å¥å±è¶å¤§
    """
    try:
        # extract the used quantities (to enable type-hinting)
        sensor: ContactSensor = env.scene[sensor_cfg.name]
        
        # get contact forces
        contact_forces = sensor.data.net_forces_w_history  # ä¸çåæ ç³»ä¸çæ¥è§¦å
        
        # 1. æ¥è§¦åå¤§å°æ£æ¥
        contact_force_norm = torch.norm(contact_forces.view(env.num_envs, -1, 3), dim=-1)
        total_contact_force = torch.sum(contact_force_norm, dim=-1)  # æ¯ä¸ªç¯å¢çæ»æ¥è§¦å
        
        # ãå³é®ãåªå¤çåçèå´åçæ¥è§¦å
        valid_contact_mask = (total_contact_force >= min_force) & (total_contact_force <= max_force)
        
        # 2. ãæ ¸å¿ãZååï¼æ³ååï¼æ£æ¥ - å¤æ­æ¯å¦ä¸»è¦ä¸ºåä¸çæ¯æå
        if contact_forces.shape[-1] == 3:  # ç¡®ä¿æ3Dååé
            contact_forces_3d = contact_forces.view(env.num_envs, -1, 3)
            
            # è®¡ç®Zæ¹å(åä¸)çååé
            total_force_z = torch.sum(contact_forces_3d[:, :, 2], dim=1)  # Zæ¹åæ»å
            
            # ç¡®ä¿Zååä¸ºæ­£å¼ï¼åä¸æ¯æï¼å¹¶è®¡ç®æ¯ä¾
            upward_force_z = torch.clamp(total_force_z, 0.0, float('inf'))  # åªèèåä¸çå
            force_z_ratio = upward_force_z / (total_contact_force + 1e-6)  # åä¸Zååæ¯ä¾
            
            # ãæ°ç­ç¥ãZååä¸å¥å±ææ­£æ¯ä¾ï¼æ¯ä¾è¶é«å¥å±è¶å¤§
            # ä½¿ç¨çº¿æ§ç¼©æ¾ï¼ä» min_z_ratio å¼å§ç»å¥å±ï¼å° 1.0 æ¶è¾¾å°æå¤§å¥å±
            z_force_quality = torch.clamp(
                (force_z_ratio - min_z_ratio) / (1.0 - min_z_ratio),  # çº¿æ§æ å°å° [0, 1]
                0.0, 1.0
            )  # ä» min_z_ratio å¼å§ç»å¥å±ï¼æ¯ä¾è¶é«å¥å±è¶å¤§
        else:
            z_force_quality = torch.ones(env.num_envs, device=env.device) * 0.5
        
        # 3. æ¥è§¦åå¤§å°è´¨éå¥å± (å¨ç®æ åéè¿æé«)
        force_error = torch.abs(total_contact_force - target_force) / (target_force * 0.5)
        contact_magnitude_quality = torch.exp(-0.5 * torch.square(force_error))
        
        # 4. ç®åçç»¼åå¥å±ï¼æ¥è§¦åå¤§å°è´¨é Ã Zååè´¨é
        # ä¸¤ä¸ªæ¡ä»¶é½æ»¡è¶³æè½è·å¾é«å¥å±
        combined_reward = contact_magnitude_quality * z_force_quality
        
        # åªå¯¹æææ¥è§¦ç»å¥å±
        foot_reward = torch.where(valid_contact_mask, combined_reward, torch.zeros_like(combined_reward))
        
        return foot_reward
        
    except (KeyError, RuntimeError):
        # Fallback: if any error occurs, return zero reward
        return torch.zeros(env.num_envs, device=env.device)


def gait_reward(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """Reward for true alternating gait pattern based on state transitions."""
    # Get contact sensor data for both feet
    try:
        left_sensor: ContactSensor = env.scene["contact_forces_LF"]
        right_sensor: ContactSensor = env.scene["contact_forces_RF"]
        
        # Get contact forces
        left_forces = left_sensor.data.net_forces_w_history
        right_forces = right_sensor.data.net_forces_w_history
        
        # Compute contact force magnitude for each foot
        left_contact_norm = torch.norm(left_forces.view(env.num_envs, -1, 3), dim=-1)
        right_contact_norm = torch.norm(right_forces.view(env.num_envs, -1, 3), dim=-1)
        
        # Determine if each foot is in contact (above threshold)
        left_in_contact = (torch.sum(left_contact_norm, dim=-1) > 1.0).float()
        right_in_contact = (torch.sum(right_contact_norm, dim=-1) > 1.0).float()
        
        # Initialize gait history in environment if not exists
        if not hasattr(env, '_gait_history'):
            env._gait_history = {
                'prev_left': torch.zeros(env.num_envs, device=env.device),
                'prev_right': torch.zeros(env.num_envs, device=env.device),
                'last_transition_step': torch.zeros(env.num_envs, device=env.device, dtype=torch.long),
                'current_step': torch.zeros(1, device=env.device, dtype=torch.long)
            }
        
        # Get previous contact states
        prev_left = env._gait_history['prev_left']
        prev_right = env._gait_history['prev_right']
        last_transition = env._gait_history['last_transition_step']
        current_step = env._gait_history['current_step']
        
        # Detect state transitions (contact state changes)
        left_changed = torch.abs(left_in_contact - prev_left) > 0.5
        right_changed = torch.abs(right_in_contact - prev_right) > 0.5
        
        # True alternating gait: when one foot changes state, the other should be stable
        # AND the change should create an alternating pattern (one up, one down)
        state_transition = left_changed | right_changed
        alternating_pattern = torch.abs(left_in_contact - right_in_contact) > 0.5
        
        # Reward for proper alternating transitions
        proper_transition = state_transition & alternating_pattern
        
        # Additional rewards for:
        # 1. Having at least one foot in contact (stability)
        stability_reward = torch.clamp(left_in_contact + right_in_contact, 0.0, 1.0)
        
        # Update transition tracking
        transition_mask = proper_transition
        last_transition[transition_mask] = current_step[0]
        
        # Penalize if no transition for too long (static gait)
        steps_since_transition = current_step[0] - last_transition
        static_penalty = (steps_since_transition > 50).float() * -0.5
        
        # Update history for next step
        env._gait_history['prev_left'] = left_in_contact.clone()
        env._gait_history['prev_right'] = right_in_contact.clone()
        env._gait_history['current_step'] += 1
        
        # Reset history for environments that just reset
        if hasattr(env, '_reset_env_ids') and len(env._reset_env_ids) > 0:
            env._gait_history['prev_left'][env._reset_env_ids] = 0.0
            env._gait_history['prev_right'][env._reset_env_ids] = 0.0
            env._gait_history['last_transition_step'][env._reset_env_ids] = current_step[0]
        
        # Combine rewards: transition reward + stability - static penalty
        total_reward = proper_transition.float() * 2.0 + stability_reward * 0.1 + static_penalty
        
        return total_reward
        
    except KeyError:
        # Fallback: if contact sensors not available, return zero reward
        return torch.zeros(env.num_envs, device=env.device)


def foot_clearance_reward(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    min_clearance: float = 0.02,  # 2cm minimum foot clearance
) -> torch.Tensor:
    """Reward for lifting feet during swing phase."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    
    # Get positions of foot links (assume last links in each leg are feet)
    # This is a simplified version - you might need to adjust based on your robot structure
    
    # For now, use joint velocities as a proxy for foot movement
    # Higher joint velocities indicate more dynamic movement (walking vs standing)
    joint_vels = asset.data.joint_vel
    
    # Focus on leg joints (exclude neck joints)
    leg_joint_vels = joint_vels[:, :10]  # Assuming first 10 joints are legs
    
    # Reward for moderate joint velocities (not too fast, not too slow)
    vel_magnitude = torch.norm(leg_joint_vels, dim=1)
    
    # Optimal velocity range for walking
    optimal_vel = 2.0
    vel_reward = torch.exp(-torch.square(vel_magnitude - optimal_vel) / 2.0)
    
    return vel_reward


def step_length_reward(
    env: ManagerBasedRLEnv,
    command_name: str = "base_velocity",
    min_step_length: float = 0.02,  # éä½æå°æ­¥é¿å° 2cm (æ´å®½æ¾)
    target_step_length: float = 0.08,  # éä½ç®æ æ­¥é¿å° 8cm (æ´ç°å®)
    max_step_length: float = 0.2,  # éä½æå¤§æ­¥é¿å° 20cm
) -> torch.Tensor:
    """
    Reward for taking proper step lengths - å¥å±ä¸æ­¥é¿æææ°æ­£æ¯ï¼è¶é¿å¥å±è¶å¤§.
    
    å¥å±ç­ç¥ï¼ææ°å¢é¿è®¾è®¡ï¼:
    1. å¾®æ­¥æ©ç½: æ­¥é¿ < min_step_length æ¶ç»äºè½»å¾®æ©ç½
    2. ææ°æ­¥é¿å¥å±: reward = exp(step_length / target_step_length) - 1 (ä¸æ­¥é¿æææ°æ­£æ¯)
    3. è¶å¤§æ­¥ææ°å¥å±: æ­¥é¿è¶è¿ç®æ æ¶ç»äºææ°å¢é¿çé¢å¤å¥å±
    4. åè¿æ¹åææ°å¥å±: å¯¹xæ¹ååè¿è·ç¦»ç»äºææ°å¥å±
    
    ææ°è®¾è®¡çä¼å¿:
    - å°æ­¥é¿: å¥å±è¾å° (ä¾å¦: 0.5åç®æ æ­¥é¿ â å¥å±â0.65)
    - ç®æ æ­¥é¿: å¥å±éä¸­ (ä¾å¦: 1.0åç®æ æ­¥é¿ â å¥å±â1.72)
    - å¤§æ­¥é¿: å¥å±å¿«éå¢é¿ (ä¾å¦: 2.0åç®æ æ­¥é¿ â å¥å±â6.39)
    
    è¿ç§è®¾è®¡å¼ºçé¼å±æºå¨äººè¿æ´å¤§çæ­¥èéå°ç¢æ­¥ã
    """
    try:
        # Get contact sensor data
        left_sensor: ContactSensor = env.scene["contact_forces_LF"]
        right_sensor: ContactSensor = env.scene["contact_forces_RF"]
        
        # Get contact state
        left_forces = left_sensor.data.net_forces_w_history
        right_forces = right_sensor.data.net_forces_w_history
        
        left_contact_norm = torch.norm(left_forces.view(env.num_envs, -1, 3), dim=-1)
        right_contact_norm = torch.norm(right_forces.view(env.num_envs, -1, 3), dim=-1)
        
        left_in_contact = (torch.sum(left_contact_norm, dim=-1) > 1.0).float()
        right_in_contact = (torch.sum(right_contact_norm, dim=-1) > 1.0).float()
        
        # Get robot base position for step length calculation
        asset: Articulation = env.scene["robot"]
        current_pos = asset.data.root_pos_w[:, :2]  # x, y position only
        
        # Get commanded velocity to determine expected movement
        cmd = env.command_manager.get_command(command_name)
        speed_cmd = torch.norm(cmd[:, :2], dim=1)  # linear velocity magnitude
        
        # Initialize step length tracking
        if not hasattr(env, '_step_length_history'):
            env._step_length_history = {
                'prev_left': torch.zeros(env.num_envs, device=env.device),
                'prev_right': torch.zeros(env.num_envs, device=env.device),
                'step_start_pos': torch.zeros((env.num_envs, 2), device=env.device),
                'last_step_pos': torch.zeros((env.num_envs, 2), device=env.device),
                'step_in_progress': torch.zeros(env.num_envs, device=env.device, dtype=torch.bool),
                'debug_counter': torch.zeros(1, device=env.device, dtype=torch.long),  # æ·»å è°è¯è®¡æ°å¨
            }
        
        history = env._step_length_history
        
        # Detect step initiation (foot lifts off)
        left_lift_off = (history['prev_left'] > left_in_contact).float()
        right_lift_off = (history['prev_right'] > right_in_contact).float()
        
        # Detect step completion (foot touches down)
        left_touch_down = (left_in_contact > history['prev_left']).float()
        right_touch_down = (right_in_contact > history['prev_right']).float()
        
        # Track step initiation
        step_initiated = (left_lift_off + right_lift_off) > 0.5
        step_completed = (left_touch_down + right_touch_down) > 0.5
        
        # è°è¯ä¿¡æ¯ï¼æ¯100æ­¥æå°ä¸æ¬¡ç¶æ
        history['debug_counter'] += 1
        if history['debug_counter'] % 100 == 0:
            num_active_steps = torch.sum(history['step_in_progress']).item()
            num_initiated = torch.sum(step_initiated).item()
            num_completed = torch.sum(step_completed).item()
            print(f"Step debug - Active: {num_active_steps}, Initiated: {num_initiated}, Completed: {num_completed}")
        
        # Update step start position when step begins
        start_new_step = step_initiated & (~history['step_in_progress'])
        history['step_start_pos'] = torch.where(
            start_new_step.unsqueeze(1),
            current_pos,
            history['step_start_pos']
        )
        history['step_in_progress'] = torch.where(start_new_step, True, history['step_in_progress'])
        
        # Calculate step length when step completes
        step_rewards = torch.zeros(env.num_envs, device=env.device)
        
        # Only calculate reward for completed steps
        completed_step_mask = step_completed & history['step_in_progress']
        
        if completed_step_mask.any():
            # Calculate actual step length (distance traveled)
            step_distance = torch.norm(current_pos - history['step_start_pos'], dim=1)
            
            # è°è¯ï¼æå°æ­¥é¿ä¿¡æ¯
            if history['debug_counter'] % 100 == 0:
                completed_distances = step_distance[completed_step_mask]
                if len(completed_distances) > 0:
                    avg_dist = torch.mean(completed_distances).item()
                    min_dist = torch.min(completed_distances).item()
                    max_dist = torch.max(completed_distances).item()
                    print(f"Step distances - Avg: {avg_dist:.3f}m, Min: {min_dist:.3f}m, Max: {max_dist:.3f}m")
            
            # Reward based on step length quality
            # 1. æ´æ¸©åçå¾®æ­¥æ©ç½
            micro_step_penalty = torch.where(
                step_distance < min_step_length,
                torch.full_like(step_distance, -0.2),  # è¿ä¸æ­¥åå°æ©ç½ï¼é¿åæå¶å­¦ä¹ 
                torch.zeros_like(step_distance)
            )
            
            # 2. ãå³é®æ¹è¿ãææ°æ­¥é¿å¥å± - ä¸æ­¥é¿æææ°æ­£æ¯ï¼è¶é¿å¥å±è¶å¤§
            # ä½¿ç¨ææ°å½æ°ï¼reward = exp(step_length / target_step_length) - 1
            # è¿æ ·å¯ä»¥ç¡®ä¿æ­¥é¿è¶å¤§ï¼å¥å±å¢é¿è¶å¿«
            exponential_step_reward = torch.exp(
                torch.clamp(step_distance / target_step_length, 0.0, 4.0)  # éå¶æå¤§ææ°é¿åæ°å¼æº¢åº
            ) - 1.0  # åå»1ä½¿å¾0æ­¥é¿æ¶å¥å±ä¸º0
            
            # 3. è¶å¤§æ­¥é¢å¤ææ°å¥å±ï¼æ­¥é¿è¶è¿ç®æ æ¶ç»äºææ°å¢é¿çé¢å¤å¥å±
            super_step_bonus = torch.where(
                step_distance > target_step_length,
                torch.exp(torch.clamp((step_distance - target_step_length) / target_step_length, 0.0, 2.0)) - 1.0,  # ææ°å¢é¿çè¶å¤§æ­¥å¥å±
                torch.zeros_like(step_distance)
            )
            
            # 4. åè¿æ¹åææ°å¥å± (é¼å±xæ¹åçå¤§æ­¥åè¿)
            forward_progress = torch.abs(current_pos[:, 0] - history['step_start_pos'][:, 0])  # x-direction progress
            forward_exponential_bonus = torch.exp(
                torch.clamp(forward_progress / target_step_length, 0.0, 3.0)
            ) - 1.0  # åè¿è·ç¦»çææ°å¥å±
            forward_exponential_bonus *= 0.3  # ç¼©æ¾ç³»æ°
            
            # 5. Scale reward by movement command (don't reward big steps when standing still)
            movement_scale = torch.clamp(speed_cmd, 0.3, 1.0)  # æé«æå°ç¼©æ¾å¼ï¼ç¡®ä¿æè¿å¨å½ä»¤æ¶æç»äºå¤§å¥å±
            
            # Combine all components (å¼ºè°ææ°æ­¥é¿å¥å±ï¼æ­¥é¿è¶å¤§å¥å±è¶å¤§)
            total_step_reward = micro_step_penalty + (exponential_step_reward + super_step_bonus + forward_exponential_bonus) * movement_scale
            
            # Apply reward only to environments that completed a step
            step_rewards = torch.where(
                completed_step_mask,
                total_step_reward,
                torch.zeros_like(total_step_reward)
            )
        
        # Reset step tracking for completed steps
        history['step_in_progress'] = torch.where(completed_step_mask, False, history['step_in_progress'])
        history['last_step_pos'] = torch.where(
            completed_step_mask.unsqueeze(1),
            current_pos,
            history['last_step_pos']
        )
        
        # Update previous contact state
        history['prev_left'] = left_in_contact.clone()
        history['prev_right'] = right_in_contact.clone()
        
        # Reset history for environments that just reset
        if hasattr(env, '_reset_env_ids') and len(env._reset_env_ids) > 0:
            history['prev_left'][env._reset_env_ids] = 0.0
            history['prev_right'][env._reset_env_ids] = 0.0
            history['step_start_pos'][env._reset_env_ids] = current_pos[env._reset_env_ids]
            history['last_step_pos'][env._reset_env_ids] = current_pos[env._reset_env_ids]
            history['step_in_progress'][env._reset_env_ids] = False
        
        return step_rewards
        
    except KeyError:
        return torch.zeros(env.num_envs, device=env.device)


def air_time_reward(
    env: ManagerBasedRLEnv,
    command_name: str = "base_velocity",
    threshold: float = 0.1,  # æå°éåº¦éå¼ï¼ä½äºæ­¤éåº¦ä¸éè¦æ¬è
    min_air_time: float = 5.0,  # æå°ç©ºä¸­æ¶é´ï¼æ§å¶æ­¥æ°ï¼ï¼çº¦0.1ç§@50Hz
    target_air_time: float = 15.0,  # ç®æ ç©ºä¸­æ¶é´ï¼æ§å¶æ­¥æ°ï¼ï¼çº¦0.3ç§@50Hz
) -> torch.Tensor:
    """
    Reward for sustained foot air time with exponential scaling.
    
    å¥å±ç­ç¥ï¼ææ°æç»­æ¶é´è®¾è®¡ï¼:
    1. è·è¸ªæ¯åªèçè¿ç»­ç©ºä¸­æ¶é´
    2. ææ°å¥å±å¬å¼: reward = exp(air_duration / target_air_time) - 1
    3. é¼å±æ´é¿çæå¨ç¸ï¼æå¶é«é¢å¾®æ¬è
    4. ç¡®ä¿è³å°ä¸åªèä¿ææ¥è§¦ä»¥ç»´æç¨³å®æ§
    
    ææ°è®¾è®¡çä¼å¿:
    - ç­ææ¬è: å¥å±å¾å° (5æ­¥ç©ºä¸­æ¶é´ â å°å¥å±)
    - éä¸­æå¨: å¥å±éä¸­ (15æ­¥ç©ºä¸­æ¶é´ â æ åå¥å±)
    - é¿æå¨: å¥å±å¿«éå¢é¿ (30æ­¥ç©ºä¸­æ¶é´ â å¤§å¥å±)
    """
    # Get commanded velocity to determine if robot should be walking
    cmd = env.command_manager.get_command(command_name)
    speed_cmd = torch.norm(cmd[:, :2], dim=1)  # linear velocity magnitude
    
    # Only reward air time when there's a movement command
    should_walk = (speed_cmd > threshold).float()
    
    try:
        # Get contact sensor data for both feet
        left_sensor: ContactSensor = env.scene["contact_forces_LF"]
        right_sensor: ContactSensor = env.scene["contact_forces_RF"]
        
        # Get contact forces
        left_forces = left_sensor.data.net_forces_w_history
        right_forces = right_sensor.data.net_forces_w_history
        
        # Compute contact state (1 = in contact, 0 = in air)
        left_contact_norm = torch.norm(left_forces.view(env.num_envs, -1, 3), dim=-1)
        right_contact_norm = torch.norm(right_forces.view(env.num_envs, -1, 3), dim=-1)
        
        # Determine contact state (use lower threshold for air detection)
        left_in_contact = (torch.sum(left_contact_norm, dim=-1) > 1.0).float()
        right_in_contact = (torch.sum(right_contact_norm, dim=-1) > 1.0).float()
        
        # Initialize air time tracking with exponential rewards
        if not hasattr(env, '_air_time_history'):
            env._air_time_history = {
                'left_air_duration': torch.zeros(env.num_envs, device=env.device),
                'right_air_duration': torch.zeros(env.num_envs, device=env.device),
                'prev_left_contact': torch.ones(env.num_envs, device=env.device),  # åè®¾å¼å§æ¶å¨å°é¢
                'prev_right_contact': torch.ones(env.num_envs, device=env.device),
                'debug_counter': torch.zeros(1, device=env.device, dtype=torch.long),
            }
        
        history = env._air_time_history
        
        # è®¡ç®å½åç©ºä¸­ç¶æ
        left_in_air = 1.0 - left_in_contact
        right_in_air = 1.0 - right_in_contact
        
        # æ´æ°ç©ºä¸­æç»­æ¶é´ï¼åéåæä½ï¼
        # å¦æèå¨ç©ºä¸­ï¼å¢å æç»­æ¶é´ï¼å¦ææ¥è§¦å°é¢ï¼éç½®ä¸º0
        history['left_air_duration'] = torch.where(
            left_in_air > 0.5,
            history['left_air_duration'] + 1.0,  # ç©ºä¸­æ¶å¢å è®¡æ°
            torch.zeros_like(history['left_air_duration'])  # æ¥è§¦æ¶éç½®
        )
        
        history['right_air_duration'] = torch.where(
            right_in_air > 0.5,
            history['right_air_duration'] + 1.0,  # ç©ºä¸­æ¶å¢å è®¡æ°
            torch.zeros_like(history['right_air_duration'])  # æ¥è§¦æ¶éç½®
        )
        
        # è°è¯ä¿¡æ¯ï¼æ¯100æ­¥æå°ä¸æ¬¡ç©ºä¸­æ¶é´ç»è®¡
        history['debug_counter'] += 1
        if history['debug_counter'] % 100 == 0:
            left_air_active = torch.sum(left_in_air > 0.5).item()
            right_air_active = torch.sum(right_in_air > 0.5).item()
            avg_left_duration = torch.mean(history['left_air_duration'][left_in_air > 0.5]).item() if left_air_active > 0 else 0
            avg_right_duration = torch.mean(history['right_air_duration'][right_in_air > 0.5]).item() if right_air_active > 0 else 0
            max_left_duration = torch.max(history['left_air_duration']).item()
            max_right_duration = torch.max(history['right_air_duration']).item()
            
            # æ¢ç®æç§ï¼50Hzæ§å¶é¢çï¼æ¯æ­¥0.02ç§ï¼
            avg_left_seconds = avg_left_duration * 0.02
            avg_right_seconds = avg_right_duration * 0.02
            max_left_seconds = max_left_duration * 0.02
            max_right_seconds = max_right_duration * 0.02
            
            print(f"Air time debug - L active: {left_air_active}, R active: {right_air_active}")
            print(f"Air durations - L avg: {avg_left_seconds:.3f}s ({avg_left_duration:.1f} steps), R avg: {avg_right_seconds:.3f}s ({avg_right_duration:.1f} steps)")
            print(f"Air max times - L max: {max_left_seconds:.3f}s ({max_left_duration:.1f} steps), R max: {max_right_seconds:.3f}s ({max_right_duration:.1f} steps)")
        
        # ãå³é®æ¹è¿ãææ°ç©ºä¸­æ¶é´å¥å±è®¡ç® - ä¿®å¤å·¦å³èä¸å¹³è¡¡é®é¢
        # åªæå½ç©ºä¸­æ¶é´è¶è¿æå°éå¼ä¸å¨åçèå´åæ¶æç»å¥å±
        max_reasonable_air_time = target_air_time * 2.0  # æå¤§åçç©ºä¸­æ¶é´ï¼0.6ç§
        
        left_qualified_air = torch.where(
            (history['left_air_duration'] >= min_air_time) & (history['left_air_duration'] <= max_reasonable_air_time),
            history['left_air_duration'],
            torch.zeros_like(history['left_air_duration'])
        )
        
        right_qualified_air = torch.where(
            (history['right_air_duration'] >= min_air_time) & (history['right_air_duration'] <= max_reasonable_air_time),
            history['right_air_duration'],
            torch.zeros_like(history['right_air_duration'])
        )
        
        # ãä¿®å¤ãä½¿ç¨æ´æ¸©åçå¥å±å½æ°ï¼é¿åææ°çç¸
        # ä½¿ç¨éå¶æ§ææ°ï¼reward = (1 - exp(-duration/target)) * max_reward
        # è¿æ ·å¨ç®æ æ¶é´éè¿å¥å±æé«ï¼è¶åºåä¸åå¢é¿
        max_air_reward = 2.0  # æå¤§ç©ºä¸­æ¶é´å¥å±
        
        left_exponential_reward = torch.where(
            left_qualified_air > 0,
            (1.0 - torch.exp(-left_qualified_air / target_air_time)) * max_air_reward,
            torch.zeros_like(left_qualified_air)
        )
        
        right_exponential_reward = torch.where(
            right_qualified_air > 0,
            (1.0 - torch.exp(-right_qualified_air / target_air_time)) * max_air_reward,
            torch.zeros_like(right_qualified_air)
        )
        
        # ãæ°å¢ãå·¦å³èå¹³è¡¡å¥å± - é¼å±åèè½®æµæå¨
        # è®¡ç®å·¦å³èç©ºä¸­æ¶é´çå·®å¼ï¼å·®å¼è¶å°å¥å±è¶é«
        left_air_ratio = history['left_air_duration'] / (target_air_time + 1e-6)
        right_air_ratio = history['right_air_duration'] / (target_air_time + 1e-6)
        air_time_balance = torch.exp(-torch.abs(left_air_ratio - right_air_ratio))  # å¹³è¡¡å¥å±
        balance_bonus = air_time_balance * 0.5  # å¹³è¡¡å¥å±æé
        
        # ç¨³å®æ§çº¦æï¼ç¡®ä¿è³å°ä¸åªèæ¥è§¦å°é¢
        # åªæå¨åèæå¨æ¶æç»äºå¥å±ï¼å¦ä¸åªèæ¯æï¼
        stable_left_swing = left_exponential_reward * right_in_contact  # å·¦èæå¨ï¼å³èæ¯æ
        stable_right_swing = right_exponential_reward * left_in_contact  # å³èæå¨ï¼å·¦èæ¯æ
        
        # ãæ°å¢ãä¸¥æ ¼ç¦æ­¢åèæ¬ç©º - åèé½å¨ç©ºä¸­æ¶ç»äºæ©ç½
        both_feet_airborne = (left_in_air > 0.5) & (right_in_air > 0.5)  # æ£æµåèæ¬ç©º
        airborne_penalty = torch.where(both_feet_airborne, torch.full_like(left_in_air, -2.0), torch.zeros_like(left_in_air))  # åèæ¬ç©ºæ©ç½
        
        # ãæ°å¢ãé¼å±è³å°ä¸åªèæ¥è§¦å°é¢çç¨³å®æ§å¥å±
        at_least_one_contact = (left_in_contact > 0.5) | (right_in_contact > 0.5)  # è³å°ä¸åªèæ¥è§¦
        stability_bonus = torch.where(at_least_one_contact, torch.full_like(left_in_contact, 0.1), torch.zeros_like(left_in_contact))
        
        # æ»çç©ºä¸­æ¶é´å¥å±ï¼åºç¡å¥å± + å¹³è¡¡å¥å± + ç¨³å®æ§å¥å± - åèæ¬ç©ºæ©ç½
        basic_air_reward = stable_left_swing + stable_right_swing
        total_air_reward = basic_air_reward + balance_bonus + stability_bonus + airborne_penalty
        
        # æ ¹æ®è¿å¨å½ä»¤ç¼©æ¾å¥å±
        scaled_reward = total_air_reward * should_walk * torch.clamp(speed_cmd, 0.3, 1.5)
        
        # è°è¯ï¼æå°å¥å±ç»è®¡
        if history['debug_counter'] % 100 == 0:
            num_rewarded = torch.sum(total_air_reward > 0.1).item()
            avg_reward = torch.mean(total_air_reward[total_air_reward > 0.1]).item() if num_rewarded > 0 else 0
            max_reward = torch.max(total_air_reward).item()
            
            # æ°å¢ï¼å¹³è¡¡æ§ç»è®¡
            avg_balance = torch.mean(air_time_balance).item()
            left_right_ratio = avg_left_duration / (avg_right_duration + 1e-6) if avg_right_duration > 0 else float('inf')
            
            # ãæ°å¢ãåèæ¬ç©ºç»è®¡
            num_both_airborne = torch.sum(both_feet_airborne).item()
            num_single_support = torch.sum(at_least_one_contact).item()
            airborne_percentage = (num_both_airborne / env.num_envs) * 100.0
            
            print(f"Air time rewards - Rewarded envs: {num_rewarded}, Avg reward: {avg_reward:.3f}, Max reward: {max_reward:.3f}")
            print(f"Air time balance - L/R ratio: {left_right_ratio:.2f}, Balance score: {avg_balance:.3f} (1.0=perfect)")
            print(f"Stability check - Both airborne: {num_both_airborne} ({airborne_percentage:.1f}%), Single support: {num_single_support}")
            
            # è­¦åï¼æ£æµå¼å¸¸çå·¦å³èä¸å¹³è¡¡
            if left_right_ratio > 3.0 or left_right_ratio < 0.33:
                print(f"â ï¸  WARNING: Severe foot imbalance detected! L/R ratio: {left_right_ratio:.2f}")
            if max_left_seconds > 1.0 or max_right_seconds > 1.0:
                print(f"â ï¸  WARNING: Excessive air time detected! Max L: {max_left_seconds:.3f}s, Max R: {max_right_seconds:.3f}s")
            if airborne_percentage > 5.0:
                print(f"â ï¸  WARNING: Too many robots with both feet airborne! {airborne_percentage:.1f}% > 5.0%")
        
        # Reset history for environments that just reset
        if hasattr(env, '_reset_env_ids') and len(env._reset_env_ids) > 0:
            history['left_air_duration'][env._reset_env_ids] = 0.0
            history['right_air_duration'][env._reset_env_ids] = 0.0
            history['prev_left_contact'][env._reset_env_ids] = 1.0
            history['prev_right_contact'][env._reset_env_ids] = 1.0
        
        return scaled_reward
        
    except KeyError:
        # Fallback: if contact sensors not available, return zero reward
        return torch.zeros(env.num_envs, device=env.device)


def step_frequency_penalty(
    env: ManagerBasedRLEnv,
    command_name: str = "base_velocity",
    target_freq: float = 1.0,  # ç®æ æ­¥é¢ 1Hz (æ¯ç§1æ­¥ï¼æ¢èç¨³)
    penalty_threshold: float = 2.0,  # éä½å°2Hzå¼å§æ©ç½ï¼æ´ä¸¥æ ¼æ§å¶
) -> torch.Tensor:
    """Penalty for high step frequency to encourage slow, stable gait (optimized version)."""
    try:
        # Get contact sensor data
        left_sensor: ContactSensor = env.scene["contact_forces_LF"]
        right_sensor: ContactSensor = env.scene["contact_forces_RF"]
        
        # Get contact state
        left_forces = left_sensor.data.net_forces_w_history
        right_forces = right_sensor.data.net_forces_w_history
        
        left_contact_norm = torch.norm(left_forces.view(env.num_envs, -1, 3), dim=-1)
        right_contact_norm = torch.norm(right_forces.view(env.num_envs, -1, 3), dim=-1)
        
        left_in_contact = (torch.sum(left_contact_norm, dim=-1) > 1.0).float()
        right_in_contact = (torch.sum(right_contact_norm, dim=-1) > 1.0).float()
        
        # Initialize step frequency tracking (simplified)
        if not hasattr(env, '_step_freq_history'):
            env._step_freq_history = {
                'prev_left': torch.zeros(env.num_envs, device=env.device),
                'prev_right': torch.zeros(env.num_envs, device=env.device),
                'step_interval_counter': torch.zeros(env.num_envs, device=env.device),  # ç®åï¼åªè·è¸ªæ­¥é´é
                'last_step_time': torch.zeros(env.num_envs, device=env.device),
                'current_time': torch.zeros(1, device=env.device),
                'debug_counter': torch.zeros(1, device=env.device, dtype=torch.long),  # æ·»å è°è¯è®¡æ°å¨
            }
        
        history = env._step_freq_history
        
        # Detect foot strikes (transition from air to ground) - åéåæä½
        left_strike = (left_in_contact > history['prev_left']).float()
        right_strike = (right_in_contact > history['prev_right']).float()
        
        # Any step occurred (åéå)
        step_occurred = (left_strike + right_strike) > 0.5
        
        # è°è¯ä¿¡æ¯ï¼æ¯100æ­¥æå°ä¸æ¬¡æ­¥é¢ç¶æ
        history['debug_counter'] += 1
        if history['debug_counter'] % 100 == 0:
            num_steps = torch.sum(step_occurred).item()
            num_left_strikes = torch.sum(left_strike).item()
            num_right_strikes = torch.sum(right_strike).item()
            print(f"Step frequency debug - Steps: {num_steps}, L strikes: {num_left_strikes}, R strikes: {num_right_strikes}")
        
        # Update step timing (åéåæä½ï¼é¿åPythonå¾ªç¯)
        current_time = history['current_time'][0]
        
        # è®¡ç®æ­¥é´é (åªå¯¹åçæ­¥æçç¯å¢)
        step_interval = current_time - history['last_step_time']
        
        # æ´æ°ä¸æ¬¡æ­¥ææ¶é´
        history['last_step_time'] = torch.where(step_occurred, current_time, history['last_step_time'])
        
        # ç®åçé¢çä¼°è®¡ï¼åºäºæ­¥é´éçåæ° (åéå)
        # æ­¥é´éä»¥æ§å¶æ­¥ä¸ºåä½ï¼è½¬æ¢ä¸ºé¢ç (Hz)
        valid_interval_mask = (step_interval > 5.0) & (step_interval < 1000.0)  # è¿æ»¤å¼å¸¸å¼
        estimated_freq = torch.zeros_like(step_interval)
        
        # é¢ç = 1 / (é´éç§æ°) = 50 / é´éæ­¥æ° (åè®¾50Hzæ§å¶é¢ç)
        estimated_freq[valid_interval_mask] = 50.0 / step_interval[valid_interval_mask]
        
        # åªå¯¹æææ­¥æååçé´éçç¯å¢è®¡ç®æ©ç½
        active_mask = step_occurred & valid_interval_mask
        
        # è°è¯ï¼æå°æ­¥é¢ä¿¡æ¯
        if history['debug_counter'] % 100 == 0:
            active_frequencies = estimated_freq[active_mask]
            if len(active_frequencies) > 0:
                avg_freq = torch.mean(active_frequencies).item()
                min_freq = torch.min(active_frequencies).item()
                max_freq = torch.max(active_frequencies).item()
                num_high_freq = torch.sum(estimated_freq > penalty_threshold).item()
                print(f"Step frequencies - Avg: {avg_freq:.2f}Hz, Min: {min_freq:.2f}Hz, Max: {max_freq:.2f}Hz, High freq: {num_high_freq}")
        
        # Apply penalty for high frequency (åéå)
        freq_excess = torch.clamp(estimated_freq - penalty_threshold, 0.0, float('inf'))
        # å¢å¼ºæ©ç½å¼ºåº¦ï¼ä½¿ç¨å¹³æ¹æ©ç½èéçº¿æ§æ©ç½
        frequency_penalty = torch.where(active_mask, -freq_excess**2, torch.zeros_like(freq_excess))
        
        # é¢å¤å¥å±ä½é¢ç¨³å®æ­¥æ (åéå)
        stable_mask = active_mask & (estimated_freq > 0.5) & (estimated_freq <= target_freq * 1.2)
        stable_gait_bonus = torch.where(stable_mask, torch.full_like(estimated_freq, 0.5), torch.zeros_like(estimated_freq))
        
        # è°è¯ï¼æå°å¥å±åæ
        if history['debug_counter'] % 100 == 0:
            num_penalized = torch.sum(frequency_penalty < 0).item()
            num_rewarded = torch.sum(stable_gait_bonus > 0).item()
            total_penalty = torch.sum(frequency_penalty).item()
            total_bonus = torch.sum(stable_gait_bonus).item()
            print(f"Step freq rewards - Penalized: {num_penalized}, Rewarded: {num_rewarded}, Total penalty: {total_penalty:.3f}, Total bonus: {total_bonus:.3f}")
        
        # Update history (åéå)
        history['prev_left'] = left_in_contact.clone()
        history['prev_right'] = right_in_contact.clone()
        history['current_time'] += 1
        
        # Reset history for environments that just reset (åéå)
        if hasattr(env, '_reset_env_ids') and len(env._reset_env_ids) > 0:
            history['prev_left'][env._reset_env_ids] = 0.0
            history['prev_right'][env._reset_env_ids] = 0.0
            history['last_step_time'][env._reset_env_ids] = current_time
        
        total_reward = frequency_penalty + stable_gait_bonus
        return total_reward
        
    except KeyError:
        return torch.zeros(env.num_envs, device=env.device)


def step_frequency_reward(
    env: ManagerBasedRLEnv,
    command_name: str = "base_velocity",
    target_freq: float = 2.0,  # ç®æ æ­¥é¢ 2Hz (æ¯ç§2æ­¥)
) -> torch.Tensor:
    """Reward for maintaining appropriate step frequency based on speed."""
    # Get commanded velocity
    cmd = env.command_manager.get_command(command_name)
    speed_cmd = torch.norm(cmd[:, :2], dim=1)
    
    try:
        # Get contact sensor data
        left_sensor: ContactSensor = env.scene["contact_forces_LF"]
        right_sensor: ContactSensor = env.scene["contact_forces_RF"]
        
        # Get contact state
        left_forces = left_sensor.data.net_forces_w_history
        right_forces = right_sensor.data.net_forces_w_history
        
        left_contact_norm = torch.norm(left_forces.view(env.num_envs, -1, 3), dim=-1)
        right_contact_norm = torch.norm(right_forces.view(env.num_envs, -1, 3), dim=-1)
        
        left_in_contact = (torch.sum(left_contact_norm, dim=-1) > 1.0).float()
        right_in_contact = (torch.sum(right_contact_norm, dim=-1) > 1.0).float()
        
        # Track contact state changes to estimate step frequency
        if not hasattr(step_frequency_reward, '_prev_left_contact'):
            step_frequency_reward._prev_left_contact = left_in_contact.clone()
            step_frequency_reward._prev_right_contact = right_in_contact.clone()
            step_frequency_reward._step_counter = torch.zeros_like(left_in_contact)
        
        # Detect foot strikes (transition from air to ground)
        left_strike = (left_in_contact > step_frequency_reward._prev_left_contact).float()
        right_strike = (right_in_contact > step_frequency_reward._prev_right_contact).float()
        
        # Count steps
        step_frequency_reward._step_counter += left_strike + right_strike
        
        # Update previous state
        step_frequency_reward._prev_left_contact = left_in_contact.clone()
        step_frequency_reward._prev_right_contact = right_in_contact.clone()
        
        # Simple reward based on movement and step occurrence
        movement_reward = torch.clamp(speed_cmd, 0.0, 1.0) * (left_strike + right_strike)
        
        return movement_reward
        
    except KeyError:
        return torch.zeros(env.num_envs, device=env.device)


def hip_abduction_reward(env: ManagerBasedRLEnv,
                         target_angle: float = 0.0,
                         asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Reward for keeping hip abduction joints (leg_l1, leg_r1) close to target angle to prevent pigeon-toed gait."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    
    # Get all joint names and find hip abduction joint indices (only once globally)
    if not hasattr(hip_abduction_reward, '_hip_indices'):
        joint_names = asset.data.joint_names
        hip_indices = []
        
        # Find hip abduction joint indices (leg_x1 joints)
        for i, name in enumerate(joint_names):
            if name in ["leg_l1_joint", "leg_r1_joint"]:
                hip_indices.append(i)
                print(f"Found hip abduction joint: {name} at index {i}")
        
        # Cache the indices globally
        hip_abduction_reward._hip_indices = hip_indices
        
        if len(hip_indices) == 0:
            print("Warning: No hip abduction joints found!")
    
    # Use cached indices
    hip_indices = hip_abduction_reward._hip_indices
    
    # If no hip joints found, return zero reward
    if len(hip_indices) == 0:
        return torch.zeros(env.num_envs, device=env.device)
    
    # Get current hip abduction positions
    hip_positions = asset.data.joint_pos[:, hip_indices]
    
    # Calculate deviation from target angle (0.0 for parallel legs)
    hip_error = torch.sum(torch.square(hip_positions - target_angle), dim=1)
    
    # Return exponential reward (higher reward for smaller error)
    return torch.exp(-10.0 * hip_error)


def track_base_height_exp(env: ManagerBasedRLEnv,
                          target_height: float = 0.4,
                          std: float = 0.1,
                          asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Reward for tracking target base height using exponential kernel."""
    # extract the used quantities (to enable type-hinting)
    asset: Articulation = env.scene[asset_cfg.name]
    
    # get current height
    current_height = asset.data.root_pos_w[:, 2]
    
    # compute height error
    height_error = torch.square(current_height - target_height)
    
    # return exponential reward (higher reward for smaller error)
    return torch.exp(-height_error / std**2)


def gait_phase_reward(env: ManagerBasedRLEnv,
                      gait_period: float = 0.75,
                      std: float = 0.5,
                      asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
    """Reward for following reference gait trajectory.
    
    Args:
        env: The RL environment.
        gait_period: Period of the reference gait cycle in seconds (default: 0.75s).
        std: Standard deviation for exponential kernel (default: 0.5 rad).
        asset_cfg: Asset configuration.
    
    Returns:
        Exponential reward based on joint angle tracking error.
    """
    # Reference gait trajectory: 18 keyframes x 10 leg joints (degrees)
    # Joint order: L1, L2, L3, L4, L5, R1, R2, R3, R4, R5
    # Time interval: 0.0417s (approximately 24Hz sampling)
    reference_gait_deg = torch.tensor([
        [0, -15, -45, 0, 60, 0, 15, 45, 0, -60],
        [0, -10.6, -37.5, -7.5, 52.5, 0, 17.3, 48.8, 3.8, -60],
        [0, -6.2, -30, -15, 45, 0, 19.6, 52.5, 7.5, -60],
        [0, -1.9, -22.5, -22.5, 37.5, 0, 21.9, 56.2, 11.2, -60],
        [0, 2.5, -15, -30, 30, 0, 24.2, 60, 15, -60],
        [0, 6.9, -7.5, -37.5, 22.5, 0, 17.3, 48.8, 3.8, -45],
        [0, 11.2, 0, -45, 15, 0, 10.4, 37.5, -7.5, -30],
        [0, 15.6, 7.5, -52.5, 7.5, 0, 3.5, 26.2, -18.8, -15],
        [0, 20, 15, -60, 0, 0, -3.5, 15, -30, 0],
        [0, 17.3, 18.8, -52.5, -3.8, 0, -6.2, 7.5, -22.5, 15],
        [0, 14.6, 22.5, -45, -7.5, 0, -8.8, 0, -15, 30],
        [0, 11.9, 26.2, -37.5, -11.2, 0, -11.5, -7.5, -7.5, 45],
        [0, 9.2, 30, -30, -15, 0, -14.2, -15, 0, 60],
        [0, 12.7, 33.8, -22.5, -18.8, 0, -10.4, -7.5, 7.5, 52.5],
        [0, 16.2, 37.5, -15, -22.5, 0, -6.5, 0, 15, 45],
        [0, 19.6, 41.2, -7.5, -26.2, 0, -2.7, 7.5, 22.5, 37.5],
        [0, 23.1, 45, 0, -30, 0, 1.2, 15, 30, 30],
        [0, 11.5, 15, 0, 7.5, 0, 8.1, 30, 15, -7.5]
    ], device=env.device, dtype=torch.float32)
    
    # Convert reference from degrees to radians
    reference_gait_rad = reference_gait_deg * (torch.pi / 180.0)
    
    # Extract the asset
    asset: Articulation = env.scene[asset_cfg.name]
    
    # Get leg joint indices (cache them on first call)
    if not hasattr(gait_phase_reward, '_leg_joint_indices'):
        joint_names = asset.data.joint_names
        leg_joints = ["leg_l1_joint", "leg_l2_joint", "leg_l3_joint", "leg_l4_joint", "leg_l5_joint",
                      "leg_r1_joint", "leg_r2_joint", "leg_r3_joint", "leg_r4_joint", "leg_r5_joint"]
        
        leg_indices = []
        for target_joint in leg_joints:
            for i, name in enumerate(joint_names):
                if name == target_joint:
                    leg_indices.append(i)
                    break
        
        if len(leg_indices) != 10:
            print(f"â ï¸  WARNING: Found only {len(leg_indices)} leg joints, expected 10!")
            print(f"Available joints: {joint_names}")
            print(f"Found indices: {leg_indices}")
        
        gait_phase_reward._leg_joint_indices = torch.tensor(leg_indices, device=env.device, dtype=torch.long)
        gait_phase_reward._initialized = True
        print(f"ð¯ Gait phase reward initialized with leg joint indices: {leg_indices}")
        print(f"   Joint names: {[joint_names[i] for i in leg_indices]}")
    
    leg_indices = gait_phase_reward._leg_joint_indices
    
    # Get current leg joint positions [num_envs, 10]
    current_joint_pos = asset.data.joint_pos[:, leg_indices]
    
    # Calculate phase in gait cycle for each environment
    # Use episode time to determine phase
    time_in_cycle = torch.fmod(env.episode_length_buf.float() * env.step_dt, gait_period)
    phase_ratio = time_in_cycle / gait_period  # [num_envs], range [0, 1)
    
    # Convert phase to keyframe index (0 to 17)
    keyframe_idx = (phase_ratio * 18.0).long()  # [num_envs]
    keyframe_idx = torch.clamp(keyframe_idx, 0, 17)  # Ensure valid range
    
    # Get reference joint angles for current phase [num_envs, 10]
    reference_pos = reference_gait_rad[keyframe_idx]
    
    # Calculate tracking error
    # Use mean squared error per joint (averaged across joints) for better scaling
    joint_diff = current_joint_pos - reference_pos  # [num_envs, 10]
    mse_per_joint = torch.mean(torch.square(joint_diff), dim=1)  # [num_envs], averaged across 10 joints
    
    # Calculate reward using exponential kernel
    # With std=3.0, mse=1.0 gives reward â 0.90, mse=5.0 gives reward â 0.57
    reward = torch.exp(-mse_per_joint / (std ** 2))
    
    # Debug print for first few calls or every 500 steps
    if not hasattr(gait_phase_reward, '_debug_count'):
        gait_phase_reward._debug_count = 0
    
    gait_phase_reward._debug_count += 1
    
    if gait_phase_reward._debug_count <= 5 or gait_phase_reward._debug_count % 500 == 0:
        avg_error = mse_per_joint.mean().item()
        avg_reward = reward.mean().item()
        max_reward = reward.max().item()
        min_reward = reward.min().item()
        avg_phase = phase_ratio.mean().item()
        
        print(f"ð¯ Gait Reward [{gait_phase_reward._debug_count}] - "
              f"MSE/joint: {avg_error:.4f}, Reward: {avg_reward:.4f} (min:{min_reward:.4f}, max:{max_reward:.4f}), "
              f"Phase: {avg_phase:.2f}, Keyframe: {keyframe_idx[0].item()}")
        
        if gait_phase_reward._debug_count <= 2:
            print(f"   Current joints (env 0, deg): {(current_joint_pos[0][:5] * 180/torch.pi).cpu().numpy()}")
            print(f"   Reference joints (env 0, deg): {(reference_pos[0][:5] * 180/torch.pi).cpu().numpy()}")
            print(f"   Joint diff (env 0, deg): {(joint_diff[0][:5] * 180/torch.pi).cpu().numpy()}")
    
    # Return exponential reward (higher reward for smaller error)
    return reward

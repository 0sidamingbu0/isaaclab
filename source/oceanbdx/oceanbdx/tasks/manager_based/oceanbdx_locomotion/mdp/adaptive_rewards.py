# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""
è‡ªé€‚åº”æ­¥æ€å¥–åŠ±å‡½æ•° (Adaptive Gait Reward Functions)
åŸºäºDisney BDXè®­ç»ƒæŒ‡å—çš„14ä¸ªæ ¸å¿ƒå¥–åŠ±
å‚è€ƒ: legged_gym, walk-these-ways, isaac-orbit
"""

from __future__ import annotations

import torch
import math
from typing import TYPE_CHECKING

from isaaclab.assets import Articulation
from isaaclab.managers import SceneEntityCfg
from isaaclab.sensors import ContactSensor

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv


# ============================================================
# 3.2 ä»»åŠ¡å¥–åŠ± (Task Rewards)
# ============================================================

def reward_velocity_tracking_exp(
    env: ManagerBasedRLEnv,
    command_name: str = "base_velocity",
    std: float = 0.5,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """
    é€Ÿåº¦è·Ÿè¸ªå¥–åŠ± (æŒ‡æ•°æ ¸)
    æ¥æº: legged_gym/envs/base/legged_robot.py
    
    å¥–åŠ±æœºå™¨äººè·Ÿè¸ªxyæ–¹å‘çš„çº¿é€Ÿåº¦æŒ‡ä»¤
    """
    asset: Articulation = env.scene[asset_cfg.name]
    command = env.command_manager.get_command(command_name)
    
    target_vel = command[:, :2]  # (vx, vy)
    actual_vel = asset.data.root_lin_vel_b[:, :2]
    
    error = torch.sum((actual_vel - target_vel) ** 2, dim=1)
    reward = torch.exp(-error / (std ** 2))
    
    return reward


def reward_angular_velocity_tracking(
    env: ManagerBasedRLEnv,
    command_name: str = "base_velocity",
    std: float = 0.5,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """
    è§’é€Ÿåº¦è·Ÿè¸ªå¥–åŠ±
    æ¥æº: legged_gym æ ‡å‡†å®ç°
    
    å¥–åŠ±æœºå™¨äººè·Ÿè¸ªzè½´çš„è§’é€Ÿåº¦æŒ‡ä»¤(è½¬å‘)
    """
    asset: Articulation = env.scene[asset_cfg.name]
    command = env.command_manager.get_command(command_name)
    
    target_ang_vel = command[:, 2]  # yaw rate
    actual_ang_vel = asset.data.root_ang_vel_b[:, 2]
    
    error = (actual_ang_vel - target_ang_vel) ** 2
    reward = torch.exp(-error / (std ** 2))
    
    return reward


# ============================================================
# 3.3 ç¨³å®šæ€§çº¦æŸ (Stability Constraints)
# ============================================================

def reward_orientation_penalty(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """
    å§¿æ€æƒ©ç½š (roll & pitch åº”æ¥è¿‘0)
    æ¥æº: æ‰€æœ‰åŒè¶³/å››è¶³é¡¹ç›®çš„æ ‡å‡†å®ç°
    
    æƒ©ç½šæœºå™¨äººèº¯å¹²çš„rollå’Œpitchåç¦»
    """
    asset: Articulation = env.scene[asset_cfg.name]
    
    # ä»å››å…ƒæ•°æå–rollå’Œpitch
    # quat: [w, x, y, z]
    quat = asset.data.root_quat_w
    w, x, y, z = quat[:, 0], quat[:, 1], quat[:, 2], quat[:, 3]
    
    # Roll (rotation around x-axis) - side-to-side tilt
    roll = torch.atan2(2 * (w * x + y * z), 1 - 2 * (x * x + y * y))
    
    # Pitch (rotation around y-axis) - forward/backward tilt
    sin_pitch = torch.clamp(2 * (w * y - z * x), -1.0, 1.0)
    pitch = torch.asin(sin_pitch)
    
    penalty = roll ** 2 + pitch ** 2
    
    return -penalty  # è´Ÿå€¼ = æƒ©ç½š


def reward_base_height_tracking(
    env: ManagerBasedRLEnv,
    target_height: float = 0.35,  # â¬‡ï¸ ä»0.39é™ä½åˆ°0.35 (Disney BDXå‚è€ƒå€¼,å‡å°‘è†ç›–è¿‡ç›´å’Œå‰å€¾)
    std: float = 0.1,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """
    èº¯å¹²é«˜åº¦è·Ÿè¸ª
    æ¥æº: legged_gym
    
    å¥–åŠ±æœºå™¨äººä¿æŒç›®æ ‡èº¯å¹²é«˜åº¦
    """
    asset: Articulation = env.scene[asset_cfg.name]
    actual_height = asset.data.root_pos_w[:, 2]  # zåæ ‡
    
    error = (actual_height - target_height) ** 2
    reward = torch.exp(-error / (std ** 2))
    
    return reward


# ============================================================
# 3.4 æ­¥æ€è´¨é‡çº¦æŸ (Gait Quality) - æ ¸å¿ƒé˜²ä½œå¼Š
# ============================================================

def reward_feet_alternating_contact(
    env: ManagerBasedRLEnv,
    threshold: float = 1.0,
) -> torch.Tensor:
    """
    æ”¹è¿›ç‰ˆè‡ªé€‚åº”äº¤æ›¿æ¥è§¦å¥–åŠ± - åŠ å…¥è¿‡æ¸¡ç›¸
    æ¥æº: æ”¹è¿›è‡ª legged_gym çš„ feet_air_time
    è¿™æ˜¯é˜²æ­¢æŒ¯åŠ¨ä½œå¼Šçš„æœ€å…³é”®å¥–åŠ±!
    
    ç›¸ä½åˆ’åˆ† (æ›´ç²¾ç»†):
    - phase 0.0-0.1: è¿‡æ¸¡ç›¸(åŒè„šæ”¯æ’‘,å‡†å¤‡å³è…¿æ‘†åŠ¨)
    - phase 0.1-0.4: å³è…¿æ‘†åŠ¨,å·¦è…¿æ”¯æ’‘
    - phase 0.4-0.6: è¿‡æ¸¡ç›¸(åŒè„šæ”¯æ’‘,å‡†å¤‡å·¦è…¿æ‘†åŠ¨)
    - phase 0.6-0.9: å·¦è…¿æ‘†åŠ¨,å³è…¿æ”¯æ’‘
    - phase 0.9-1.0: è¿‡æ¸¡ç›¸(åŒè„šæ”¯æ’‘,å‡†å¤‡ä¸‹ä¸€å‘¨æœŸ)
    """
    # è·å–ç›¸ä½ç®¡ç†å™¨
    if not hasattr(env, 'phase_manager'):
        return torch.zeros(env.num_envs, device=env.device)
    
    phase = env.phase_manager.current_phase
    
    # è·å–å·¦å³è„šæ¥è§¦åŠ›
    left_sensor: ContactSensor = env.scene["contact_forces_LF"]
    right_sensor: ContactSensor = env.scene["contact_forces_RF"]
    
    left_forces = left_sensor.data.net_forces_w_history
    right_forces = right_sensor.data.net_forces_w_history
    
    # è®¡ç®—æ¥è§¦åŠ›å¤§å°
    left_contact_norm = torch.norm(left_forces.view(env.num_envs, -1, 3), dim=-1)
    right_contact_norm = torch.norm(right_forces.view(env.num_envs, -1, 3), dim=-1)
    
    # äºŒå€¼åŒ–æ¥è§¦çŠ¶æ€
    left_in_contact = (torch.sum(left_contact_norm, dim=-1) > threshold).float()
    right_in_contact = (torch.sum(right_contact_norm, dim=-1) > threshold).float()
    
    reward = torch.zeros(env.num_envs, device=env.device)
    
    # ğŸ†• è¿‡æ¸¡ç›¸ 1 (0.0-0.1): å…è®¸åŒè„šç€åœ°,å‡†å¤‡å³è…¿æ‘†åŠ¨
    transition1_mask = phase < 0.1
    both_contact_ok_1 = left_in_contact * right_in_contact
    reward[transition1_mask] = both_contact_ok_1[transition1_mask] * 0.3  # è½»å¾®å¥–åŠ±
    
    # æ‘†åŠ¨ç›¸ 1 (0.1-0.4): æœŸæœ›å³è…¿æ‘†åŠ¨(ç¦»åœ°), å·¦è…¿æ”¯æ’‘(ç€åœ°)
    swing1_mask = (phase >= 0.1) & (phase < 0.4)
    ideal_state_1 = (1 - right_in_contact) * left_in_contact  # å·¦ç€å³ç¦»
    both_contact_1 = right_in_contact * left_in_contact        # åŒè„šç€åœ°(æƒ©ç½š)
    reward[swing1_mask] = ideal_state_1[swing1_mask] * 1.0 - both_contact_1[swing1_mask] * 0.8
    
    # ğŸ†• è¿‡æ¸¡ç›¸ 2 (0.4-0.6): å…è®¸åŒè„šç€åœ°,å‡†å¤‡å·¦è…¿æ‘†åŠ¨
    transition2_mask = (phase >= 0.4) & (phase < 0.6)
    both_contact_ok_2 = left_in_contact * right_in_contact
    reward[transition2_mask] = both_contact_ok_2[transition2_mask] * 0.3
    
    # æ‘†åŠ¨ç›¸ 2 (0.6-0.9): æœŸæœ›å·¦è…¿æ‘†åŠ¨, å³è…¿æ”¯æ’‘
    swing2_mask = (phase >= 0.6) & (phase < 0.9)
    ideal_state_2 = (1 - left_in_contact) * right_in_contact  # å³ç€å·¦ç¦»
    both_contact_2 = left_in_contact * right_in_contact
    reward[swing2_mask] = ideal_state_2[swing2_mask] * 1.0 - both_contact_2[swing2_mask] * 0.8
    
    # ğŸ†• è¿‡æ¸¡ç›¸ 3 (0.9-1.0): å…è®¸åŒè„šç€åœ°,å‡†å¤‡ä¸‹ä¸€å‘¨æœŸ
    transition3_mask = phase >= 0.9
    both_contact_ok_3 = left_in_contact * right_in_contact
    reward[transition3_mask] = both_contact_ok_3[transition3_mask] * 0.3
    
    return reward


def reward_stride_length_tracking(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """
    è‡ªé€‚åº”æ­¥é•¿è·Ÿè¸ª
    æ¥æº: è‡ªå®šä¹‰ï¼Œæ¦‚å¿µç±»ä¼¼ cassie-mujoco-sim
    
    å¥–åŠ±æœºå™¨äººçš„å®é™…æ­¥å¹…æ¥è¿‘æœŸæœ›æ­¥å¹…
    """
    # è·å–æœŸæœ›æ­¥å¹…
    if not hasattr(env, 'phase_manager'):
        return torch.zeros(env.num_envs, device=env.device)
    
    _, desired_stride, _ = env.phase_manager.get_current_targets()
    
    asset: Articulation = env.scene[asset_cfg.name]
    
    # è·å–å·¦å³è„šä½ç½®ï¼ˆéœ€è¦ä»body_posè·å–ï¼‰
    # ç®€åŒ–ï¼šä½¿ç”¨å…³èŠ‚ä½ç½®ä¼°ç®—è„šéƒ¨ä½ç½®
    # è¿™é‡Œå‡è®¾æœ‰body_poså­—å…¸æˆ–è€…å¯ä»¥ä»å…³èŠ‚è§’åº¦è®¡ç®—
    # ä¸ºäº†ç®€åŒ–å®ç°ï¼Œä½¿ç”¨baseå‰è¿›æ–¹å‘çš„å…³èŠ‚ä½ç½®å·®å¼‚
    joint_pos = asset.data.joint_pos
    
    # ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å·¦å³è…¿çš„é«‹å…³èŠ‚pitchè§’åº¦å·®å¼‚ä¼°ç®—æ­¥å¹…
    # å®é™…åº”è¯¥ç”¨FKè®¡ç®—è„šå°–ä½ç½®ï¼Œè¿™é‡Œç”¨ç®€åŒ–æ–¹æ³•
    # TODO: ä½¿ç”¨æ­£è¿åŠ¨å­¦è®¡ç®—ç²¾ç¡®è„šéƒ¨ä½ç½®
    
    # æš‚æ—¶è¿”å›å›ºå®šå°å¥–åŠ±ï¼Œç­‰å¾…å®Œæ•´FKå®ç°
    error = torch.abs(0.2 - desired_stride)  # å‡è®¾å½“å‰æ­¥å¹…0.2m
    reward = torch.exp(-error / 0.1)
    
    return reward * 0.5  # é™ä½æƒé‡å› ä¸ºæ˜¯ç®€åŒ–å®ç°


def reward_foot_clearance(
    env: ManagerBasedRLEnv,
    threshold: float = 1.0,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """
    è‡ªé€‚åº”è„šæŠ¬èµ·é«˜åº¦
    æ¥æº: legged_gym çš„ foot_clearance_cmd_linear
    
    å¥–åŠ±æ‘†åŠ¨è…¿æŠ¬èµ·åˆ°æœŸæœ›é«˜åº¦
    """
    # è·å–æœŸæœ›æŠ¬è„šé«˜åº¦
    if not hasattr(env, 'phase_manager'):
        return torch.zeros(env.num_envs, device=env.device)
    
    phase = env.phase_manager.current_phase
    _, _, desired_clearance = env.phase_manager.get_current_targets()
    
    # è·å–æ¥è§¦çŠ¶æ€
    left_sensor: ContactSensor = env.scene["contact_forces_LF"]
    right_sensor: ContactSensor = env.scene["contact_forces_RF"]
    
    left_contact = torch.norm(left_sensor.data.net_forces_w_history.view(env.num_envs, -1, 3), dim=-1).sum(dim=-1)
    right_contact = torch.norm(right_sensor.data.net_forces_w_history.view(env.num_envs, -1, 3), dim=-1).sum(dim=-1)
    
    left_swing = left_contact < threshold
    right_swing = right_contact < threshold
    
    # è·å–è„šé«˜åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦FKï¼‰
    # TODO: ä½¿ç”¨æ­£è¿åŠ¨å­¦è®¡ç®—ç²¾ç¡®è„šéƒ¨é«˜åº¦
    asset: Articulation = env.scene[asset_cfg.name]
    base_height = asset.data.root_pos_w[:, 2]
    
    # ç®€åŒ–ï¼šå‡è®¾è„šé«˜åº¦ = åŸºåº§é«˜åº¦ - è…¿é•¿ + æŠ¬èµ·é‡
    # çœŸå®å®ç°éœ€è¦FK
    estimated_clearance = torch.ones_like(base_height) * 0.03  # å‡è®¾3cmæŠ¬èµ·
    
    reward = torch.zeros(env.num_envs, device=env.device)
    
    # å³è…¿æ‘†åŠ¨ç›¸ (phase < 0.5)
    right_swing_mask = (phase < 0.5) & right_swing
    if right_swing_mask.any():
        clearance_achieved = estimated_clearance - desired_clearance
        reward[right_swing_mask] = torch.clamp(clearance_achieved[right_swing_mask], 0, 0.1)
    
    # å·¦è…¿æ‘†åŠ¨ç›¸
    left_swing_mask = (phase >= 0.5) & left_swing
    if left_swing_mask.any():
        clearance_achieved = estimated_clearance - desired_clearance
        reward[left_swing_mask] = torch.clamp(clearance_achieved[left_swing_mask], 0, 0.1)
    
    return reward * 0.5  # é™ä½æƒé‡å› ä¸ºæ˜¯ç®€åŒ–å®ç°


# ============================================================
# 3.5 å®‰å…¨çº¦æŸ (Safety Constraints)
# ============================================================

def reward_undesired_contacts(
    env: ManagerBasedRLEnv,
    threshold: float = 1.0,
) -> torch.Tensor:
    """
    æƒ©ç½šéè¶³éƒ¨æ¥è§¦
    æ¥æº: legged_gym æ ‡å‡†å®ç°
    
    æƒ©ç½šè†ç›–ã€å¤§è…¿ã€èº¯å¹²æ¥è§¦åœ°é¢
    """
    # æ£€æŸ¥è†ç›–æ¥è§¦
    try:
        left_knee: ContactSensor = env.scene["contact_forces_knee_L"]
        right_knee: ContactSensor = env.scene["contact_forces_knee_R"]
        
        left_knee_contact = torch.norm(left_knee.data.net_forces_w_history.view(env.num_envs, -1, 3), dim=-1).sum(dim=-1)
        right_knee_contact = torch.norm(right_knee.data.net_forces_w_history.view(env.num_envs, -1, 3), dim=-1).sum(dim=-1)
        
        penalty = ((left_knee_contact > threshold).float() + 
                  (right_knee_contact > threshold).float())
        
        return -penalty
    except KeyError:
        return torch.zeros(env.num_envs, device=env.device)


def reward_joint_limits_penalty(
    env: ManagerBasedRLEnv,
    soft_limit_ratio: float = 0.9,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """
    å…³èŠ‚é™ä½æƒ©ç½š
    æ¥æº: legged_gym/envs/base/legged_robot.py
    
    æƒ©ç½šæ¥è¿‘å…³èŠ‚é™ä½çš„åŠ¨ä½œ
    """
    asset: Articulation = env.scene[asset_cfg.name]
    
    joint_pos = asset.data.joint_pos
    # soft_joint_pos_limits shape: [num_joints, 2] or [num_envs, num_joints, 2]
    # éœ€è¦å¹¿æ’­åˆ° [num_envs, num_joints]
    joint_limits = asset.data.soft_joint_pos_limits
    if joint_limits.dim() == 2:
        # [num_joints, 2] -> [1, num_joints] for broadcasting
        joint_limits_low = joint_limits[:, 0].unsqueeze(0)
        joint_limits_high = joint_limits[:, 1].unsqueeze(0)
    else:
        # [num_envs, num_joints, 2]
        joint_limits_low = joint_limits[:, :, 0]
        joint_limits_high = joint_limits[:, :, 1]
    
    # å½’ä¸€åŒ–åˆ° [-1, 1]
    normalized_pos = 2 * (joint_pos - joint_limits_low) / \
                     (joint_limits_high - joint_limits_low + 1e-8) - 1
    
    # æƒ©ç½šè¶…å‡ºè½¯é™ä½çš„å…³èŠ‚
    out_of_soft_limits = (torch.abs(normalized_pos) > soft_limit_ratio).float()
    penalty = torch.sum(out_of_soft_limits * normalized_pos ** 2, dim=1)
    
    return -penalty


def reward_feet_slip_penalty(
    env: ManagerBasedRLEnv,
    threshold: float = 1.0,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """
    æ”¯æ’‘è…¿æ»‘åŠ¨æƒ©ç½š
    æ¥æº: legged_gym çš„ feet_slip
    
    æƒ©ç½šæ”¯æ’‘è…¿çš„æ°´å¹³é€Ÿåº¦ï¼ˆåº”è¯¥æ¥è¿‘0ï¼‰
    """
    # è·å–æ¥è§¦çŠ¶æ€
    left_sensor: ContactSensor = env.scene["contact_forces_LF"]
    right_sensor: ContactSensor = env.scene["contact_forces_RF"]
    
    left_contact = torch.norm(left_sensor.data.net_forces_w_history.view(env.num_envs, -1, 3), dim=-1).sum(dim=-1)
    right_contact = torch.norm(right_sensor.data.net_forces_w_history.view(env.num_envs, -1, 3), dim=-1).sum(dim=-1)
    
    left_in_stance = left_contact > threshold
    right_in_stance = right_contact > threshold
    
    # è·å–è„šéƒ¨é€Ÿåº¦ï¼ˆç®€åŒ–ï¼šä½¿ç”¨baseé€Ÿåº¦ï¼‰
    asset: Articulation = env.scene[asset_cfg.name]
    base_vel = asset.data.root_lin_vel_b[:, :2]  # (vx, vy)
    
    # æ”¯æ’‘è…¿çš„æ»‘åŠ¨é‡ï¼ˆç®€åŒ–å®ç°ï¼‰
    slip = torch.sum(base_vel ** 2, dim=1)
    
    left_slip = slip * left_in_stance.float()
    right_slip = slip * right_in_stance.float()
    
    return -(left_slip + right_slip)


# ============================================================
# 3.6 èƒ½è€—ä¸å¹³æ»‘æ€§ (Energy & Smoothness)
# ============================================================

def reward_action_smoothness(env: ManagerBasedRLEnv) -> torch.Tensor:
    """
    åŠ¨ä½œå¹³æ»‘æ€§å¥–åŠ±
    æ¥æº: legged_gym çš„ action_rate
    é˜²æ­¢é«˜é¢‘æŒ¯åŠ¨ä½œå¼Šçš„å…³é”®!
    """
    action_diff = env.action_manager.action - env.action_manager.prev_action
    penalty = torch.sum(action_diff ** 2, dim=1)
    
    return -penalty


def reward_joint_torque_penalty(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """
    å…³èŠ‚åŠ›çŸ©æƒ©ç½š (èƒ½è€—)
    æ¥æº: legged_gym çš„ torques
    """
    asset: Articulation = env.scene[asset_cfg.name]
    torques = asset.data.applied_torque
    penalty = torch.sum(torques ** 2, dim=1)
    
    return -penalty


def reward_joint_acceleration_penalty(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """
    å…³èŠ‚åŠ é€Ÿåº¦æƒ©ç½š
    æ¥æº: legged_gym çš„ dof_acc
    é˜²æ­¢å‰§çƒˆè¿åŠ¨å’ŒæŒ¯åŠ¨
    """
    asset: Articulation = env.scene[asset_cfg.name]
    joint_acc = asset.data.joint_acc
    penalty = torch.sum(joint_acc ** 2, dim=1)
    
    return -penalty


def reward_joint_velocity_penalty(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
) -> torch.Tensor:
    """
    å…³èŠ‚é€Ÿåº¦æƒ©ç½š
    æ¥æº: legged_gym çš„ dof_vel
    """
    asset: Articulation = env.scene[asset_cfg.name]
    joint_vel = asset.data.joint_vel
    penalty = torch.sum(joint_vel ** 2, dim=1)
    
    return -penalty


def reward_termination_penalty(env: ManagerBasedRLEnv) -> torch.Tensor:
    """
    ç»ˆæ­¢æƒ©ç½š
    ä¸¥å‰æƒ©ç½šå¯¼è‡´episodeç»ˆæ­¢çš„è¡Œä¸ºï¼ˆæ‘”å€’ç­‰ï¼‰
    """
    return -env.termination_manager.terminated.float() * 100.0


# ============================================================
# ğŸ†• é‡å¿ƒè½¬ç§»ä¸æŠ¬è…¿å¥–åŠ± (Weight Transfer & Lift)
# ============================================================

def reward_weight_transfer(
    env: ManagerBasedRLEnv,
    threshold: float = 1.0,
) -> torch.Tensor:
    """
    ğŸ†• é‡å¿ƒè½¬ç§»å¥–åŠ± - ç›¸ä½æ„ŸçŸ¥,é¼“åŠ±å•è…¿æ”¯æ’‘å’ŒæŠ¬è…¿
    
    æ ¸å¿ƒæ€æƒ³: æ ¹æ®æ­¥æ€ç›¸ä½å¥–åŠ±æ­£ç¡®çš„é‡å¿ƒè½¬ç§»
    - æ”¯æ’‘ç›¸: å¥–åŠ±æ”¯æ’‘è…¿æœ‰åŠ›æ¥è§¦ + è½»å¾®æƒ©ç½šæ‘†åŠ¨è…¿æ¥è§¦
    - æ‘†åŠ¨ç›¸: å¥–åŠ±æ‘†åŠ¨è…¿ç¦»åœ°
    - è¿‡æ¸¡ç›¸: å…è®¸åŒè„šç€åœ°(å‡†å¤‡é‡å¿ƒè½¬ç§»)
    
    ç›¸ä½åˆ’åˆ†:
    - 0.0-0.1: è¿‡æ¸¡(åŒè„šæ”¯æ’‘)
    - 0.1-0.4: å·¦è…¿æ”¯æ’‘ + å³è…¿æ‘†åŠ¨
    - 0.4-0.6: è¿‡æ¸¡(åŒè„šæ”¯æ’‘)
    - 0.6-0.9: å³è…¿æ”¯æ’‘ + å·¦è…¿æ‘†åŠ¨
    - 0.9-1.0: è¿‡æ¸¡(åŒè„šæ”¯æ’‘)
    """
    # è·å–ç›¸ä½ç®¡ç†å™¨
    if not hasattr(env, 'phase_manager'):
        return torch.zeros(env.num_envs, device=env.device)
    
    phase = env.phase_manager.current_phase
    
    # è·å–å·¦å³è„šæ¥è§¦åŠ›
    left_sensor: ContactSensor = env.scene["contact_forces_LF"]
    right_sensor: ContactSensor = env.scene["contact_forces_RF"]
    
    left_forces = left_sensor.data.net_forces_w_history
    right_forces = right_sensor.data.net_forces_w_history
    
    # è®¡ç®—æ¥è§¦åŠ›å¤§å° (å½’ä¸€åŒ–åˆ° [0, 1])
    left_contact_norm = torch.norm(left_forces.view(env.num_envs, -1, 3), dim=-1)
    right_contact_norm = torch.norm(right_forces.view(env.num_envs, -1, 3), dim=-1)
    
    # æ¥è§¦å¼ºåº¦ (0=ç¦»åœ°, 1=ç€åœ°)
    left_contact_strength = torch.clamp(torch.sum(left_contact_norm, dim=-1) / (threshold * 10), 0, 1)
    right_contact_strength = torch.clamp(torch.sum(right_contact_norm, dim=-1) / (threshold * 10), 0, 1)
    
    reward = torch.zeros(env.num_envs, device=env.device)
    
    # æ”¯æ’‘ç›¸ 1 + æ‘†åŠ¨ç›¸ 1 (0.1-0.4): å·¦è…¿æ”¯æ’‘,å³è…¿æ‘†åŠ¨
    phase1_mask = (phase >= 0.1) & (phase < 0.4)
    if phase1_mask.any():
        # å¥–åŠ±å·¦è…¿æœ‰åŠ›æ¥è§¦
        reward[phase1_mask] += left_contact_strength[phase1_mask] * 0.5
        # å¥–åŠ±å³è…¿ç¦»åœ°
        reward[phase1_mask] += (1.0 - right_contact_strength[phase1_mask]) * 0.8
        # è½»å¾®æƒ©ç½šå³è…¿ç€åœ°(é¼“åŠ±æŠ¬èµ·)
        reward[phase1_mask] -= right_contact_strength[phase1_mask] * 0.3
    
    # æ”¯æ’‘ç›¸ 2 + æ‘†åŠ¨ç›¸ 2 (0.6-0.9): å³è…¿æ”¯æ’‘,å·¦è…¿æ‘†åŠ¨
    phase2_mask = (phase >= 0.6) & (phase < 0.9)
    if phase2_mask.any():
        # å¥–åŠ±å³è…¿æœ‰åŠ›æ¥è§¦
        reward[phase2_mask] += right_contact_strength[phase2_mask] * 0.5
        # å¥–åŠ±å·¦è…¿ç¦»åœ°
        reward[phase2_mask] += (1.0 - left_contact_strength[phase2_mask]) * 0.8
        # è½»å¾®æƒ©ç½šå·¦è…¿ç€åœ°
        reward[phase2_mask] -= left_contact_strength[phase2_mask] * 0.3
    
    # è¿‡æ¸¡ç›¸: å…è®¸åŒè„šç€åœ°,ä¸ç»™å¥–åŠ±ä¹Ÿä¸æƒ©ç½š
    # (phase < 0.1 or 0.4 <= phase < 0.6 or phase >= 0.9)
    
    return reward
